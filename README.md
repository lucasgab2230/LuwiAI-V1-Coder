# ðŸ§  slm-codegen

A lightweight Small Language Model (SLM) designed specifically for code generation on legacy CPUs (e.g., Intel Ivy Bridge), trained exclusively on official documentation for popular languages and frameworks like Python, JavaScript, C++, React, Django, and more.

---

## ðŸŽ¯ Project Goals

- Run efficiently on CPUs without AVX2/AVX512 support
- Train on a focused dataset built from curated official documentation
- Output accurate, clean, and idiomatic code snippets from prompts
- Preserve syntax-aware tokenization and fast inference via quantized ONNX

---

## ðŸ—ï¸ Architecture

- Transformer-based: 4 layers, 256-dimensional embeddings, 4 heads
- Tokenizer: Byte Pair Encoding (BPE) via SentencePiece
- Output: Clean code generation with minimal formatting errors

---

## ðŸ“ Project Structure

slm-codegen/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw_docs/ # Collected official documentation (.md, .txt, .html)
â”‚ â”œâ”€â”€ processed/ # Extracted + cleaned code blocks
â”‚ â””â”€â”€ tokenized/ # Tokenized samples for training
â”‚
â”œâ”€â”€ tokenizer/
â”‚ â”œâ”€â”€ tokenizer_config.yaml # SentencePiece or BPE config
â”‚ â”œâ”€â”€ vocab.model # Trained tokenizer model
â”‚ â””â”€â”€ train_tokenizer.py # Script to train tokenizer
â”‚
â”œâ”€â”€ model/
â”‚ â”œâ”€â”€ architecture.py # Defines lightweight transformer model
â”‚ â”œâ”€â”€ config.json # Model hyperparameters
â”‚ â”œâ”€â”€ train.py # Training loop
â”‚ â””â”€â”€ export_onnx.py # ONNX export script
â”‚
â”œâ”€â”€ quantization/
â”‚ â”œâ”€â”€ quantize.py # ONNX Runtime quantization script
â”‚ â””â”€â”€ calibration_data.py # Calibration sample loader
â”‚
â”œâ”€â”€ inference/
â”‚ â”œâ”€â”€ run_onnx.py # ONNX-based inference script
â”‚ â”œâ”€â”€ runner.cpp # C++ inference (optional)
â”‚ â””â”€â”€ build.sh # g++ or CMake build script for Ivy Bridge
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ dataset_builder.py # Preprocessing and chunking helpers
â”‚ â””â”€â”€ eval.py # Code generation accuracy + lint checker
â”‚
â”œâ”€â”€ tests/
â”‚ â””â”€â”€ test_generation.py # Unit tests for model outputs
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

---

## âš™ï¸ Setup

1. **Install dependencies:**

```bash
pip install -r requirements.txt
```

2. **Prepare training data:**

```bash
python utils/dataset_builder.py
```

3. **Train the tokenizer:**

```bash
python tokenizer/train_tokenizer.py
```

4. **Train the model:**

```bash
python model/train.py
```

5. **Export to ONNX + Quantize**:

```bash
python model/export_onnx.py
python quantization/quantize.py
```

6. **Run Inference:**

```bash
python inference/run_onnx.py
```

---

## ðŸ“Š Evaluation

Evaluate perplexity and code validity:

```bash
python utils/eval.py
```

---

ðŸ§ª Tested On

Intel Ivy Bridge CPUs (no AVX2 required)

Python 3.8+

PyTorch, HuggingFace Datasets, ONNX Runtime, SentencePiece

---

ðŸ’¬ Chat with Your Model

You can now interact with your trained model in a conversational way using a simple command-line interface:

```bash
python chat/chat_cli.py
```

Type a prompt like:
`Prompt > create a python function to validate email addresses`

And receive generated code directly from your quantized ONNX model!

`> To exit the chat loop, type exit or quit.`

Make sure your tokenizer model (tokenizer/slm_tokenizer.model) and quantized ONNX model (model/slm_ivybridge_int8.onnx) are built before starting the chat.

---

ðŸ“œ License

MIT License
